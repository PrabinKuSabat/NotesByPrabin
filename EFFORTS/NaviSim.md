# Unknown Things

1. BICG benchmarks from PolyBench
2. MGPUSim
3. Performance discrepancies
4. Akita simulation engine
5. DRAMSim3
6. HBM
7. GDDR
8. Daisen GPU visualization framework
9. photolithography technology
10. Barriers in work-groups in GPUs.
11. What are work-items, work-groups, wavefront?  
	 WaveFront: Group of threads that are executed simultaneously using SIMD
12. Lock-step execution
13. Thread divergence
	 - is a phenomenon that occurs in parallel computing when different threads in a warp take different paths through the code.
	 - happens when a thread encounters a conditional statement and the condition evaluates to different values for different threads.
14. What is a Kenel in GPUs?
15. Multi2Sim
16. GPGPUSim
17. ECC(Error Correction Code)
	 - Uses hamming code
	 - Which in turn uses parity bits that are added in between the data bits to generate a ECC code
	 - Formula for deciding no. of parity bits 'r' is:  
		2^r > m + r + 1  
		m : no. of data bits  
		r : no. of parity bits to added
18. Use of L1 Instruction cache, Variable cache, Scalar cache.
19. What are cache Invalidation instructions?
	 - used to ensure data coherence across different processing units and cache levels.
	 - explicitly mark cache lines as invalid
	 - forcing subsequent accesses to fetch the data from a higher level cache or main memory.
	 - Data Coherence?
		- refers to the consistency and uniformity of data across different systems and datasets,
		- ensuring that all instances of shared data reflect the same values and logical relationships.
20. [[Cache stale]]
	 - Cache staleness refers to the condition where cached data is considered outdated or expired but is still retained in the cache for potential use.
	 - caching strategies that aim to improve application performance and reliability by allowing systems to serve stale data when fresh data is unavailable.
	 - TTL(Time to Live) decides how long the content can be served fresh.

---
---

# Architectures

## GCN3 Architecture (Graphics Core Next) `fas:Microchip`

### Drawback

L1 and L2 caches are crossbar connected.

- Though it provides low latency and high-throughput
- the design struggles to scale with the increase in the no. of CUs.  
![[Pasted image 20240912204038.png]]

## Compute Unit

- responsible for instruction execution and data processing
- 1 Scheduler  
  can fetch and issue instructions for up to 40 wavefronts.  
  40 wavefront handling
	- decode 5 instruction  
	  and issue to 5 execution units
		- 1 branch unit
		- 1 scalar unit(executes instruction manipulating data shared by work-items in a wavefront)
		- 1 LDS(Local Data Share)
		- 1 vector memory unit
		- 4 SIMD unit
- 1 SIMD unit
	- responsible for executing vectorized floating-point instruction for 10 out 40 wavefronts
	- contains 16 single-precision ALUs
	- => 64-work-item wavefront takes 4 cycles to finish  
![[Pasted image 20240912203931.png|Architecture of a GCN3 Compute Unit]]

## Cache `fas:ShippingFast`

- two-level cache hierarchy.
- L1
	- L1 scalar cache(shared with multi CUs)  
	  used for storing constant data, such as kernel arguments and pointers.
	- L1 instruction cache(shared with multi CUs)
	- L1 vector cache  
	  write-through cache that stores most of the data  
	  Each CU has a dedicated one.
	- L1 scalar and instruction cache are shared  
	  among CUs in shader array(typically 4 CUs)
- L1 `rir:ArrowLeftS` L2(write-back caches) `rir:ArrowLeftS` DRAM (HBM or GDDR)
- Both the L2 caches and DRAM controllers are banked

---

## RDNA Architecture (Radeon DNA)

Designed to replace GCN for better scalability.  
![[Pasted image 20240916145837.png]]

### Major Changes

1. Reduced size of wavefront.(64`rir:ArrowRightS`32 work items)
	- It expected the CUs to better cope with higher degree of thread divergence
	- fewer memory transactions generated by 1 load/store instr.
		- The total no. of transactions generated by kernel is same.
	- potentially reducing memory acess latencies  
2. Introduction of DCUs(Dual Compute Units) replacing the GCN CUs.  
	Dual coz Dual no. of ALUs than CUs
	- Contains 4 schedulers.
	- 1 Scheduler dispatches instructions to 1 SIMD unit. (1 `rir:ArrowRightS` 1)
	- 1 SIMD has 32 single-precision ALUs ( 2x `fas:ExpandAlt` than CU)  
	  `fas:Equals` no. of work items  
	  => each SIMD unit can execute 1 instruction in a cycle.  
		  (compared to 4 cycles in CU)
3. 3-layer structure cache hierarchy
	- Dedicated L0I and L0S caches(read-only) for each DCU.
	- 1 DCU is connected to 2 L0V caches(write-through)  
	  => 2 SIMD and 2 Schedulers use 1 L0V.  
	  - update in 1 L0 may render other as stale  
		 => coherence issues within DCU
	  - requires explicit cache invalidatio instructions(Not available in GCN ISA)
4. An Intermediate level of caching
	- new write-evict L1 cache
	- serves a group of DCUs(4-5) in a Shader Array
	- L0 `rir:ArrowLeftS` L1 `rir:ArrowLeftS` L2
	- reduces request arriving at L2 cache(in case of L1 hits)  
	  reduces amount of data transmitted from L2 to L0(arcoss chip)
	- which <mark style="background: #08BFFF99;">lowers power consumption</mark> by cross chip transfers.
	- 2x cache line size ( 64B `rir:ArrowRightS` 128B)  
	  => unique single-precision numbers for all 32 work-items.

![[Pasted image 20240916145901.png]]  

---

Comparing cache latencies between the GCN and RDNA GPUs, while running the pointer chasing microbenchmark. The results suggest that, other than the publicly announced changes in the specifications (e.g., cache sizes), AMD has made many unannounced design improvements (e.g., reducing cache latencies).

![[Pasted image 20240912203601.png]]

---
---

# NAVISIM [`ris:Navigation`](https://github.com/autonomousvision/navsim)  
**Data-Driven Non-Reactive Autonomous Vehicle Simulation and Benchmarking**
## The Akita Simulation Engine
- is a computer architecture simulator engine that is implemented in the Go programming language.
- has been used effectively in the MGPUSim simulator.
- high flexibility and optimized multi-threaded simulation performance.
- NAVISIM has been implemented independently using the Akita Simulation Engine


## RDNA ISA Emulation



---
# Tasks

- [ ] Go through the RDNA Whitepaper.
- [ ] Check [Cache Eviction](https://docs.jboss.org/jbossclustering/hibernate-caching/3.3/en-US/html/eviction.html)
