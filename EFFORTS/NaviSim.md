# Unknown Things

1. BICG benchmarks from PolyBench
2. MGPUSim
3. Performance discrepancies
4. Akita simulation engine
5. DRAMSim3
6. HBM
7. GDDR
8. Daisen GPU visualization framework
9. photolithography technology
10. Barriers in work-groups in GPUs.
11. What are work-items, work-groups, wavefront?  
	 WaveFront: Group of threads that are executed simultaneously using SIMD
12. Lock-step execution
13. Thread divergence
	 - is a phenomenon that occurs in parallel computing when different threads in a warp take different paths through the code.
	 - happens when a thread encounters a conditional statement and the condition evaluates to different values for different threads.
14. What is a Kenel in GPUs?
15. Multi2Sim
16. GPGPUSim
17. ECC(Error Correction Code)
	 - Uses hamming code
	 - Which in turn uses parity bits that are added in between the data bits to generate a ECC code
	 - Formula for deciding no. of parity bits 'r' is:  
		2^r > m + r + 1  
		m : no. of data bits  
		r : no. of parity bits to added
18. Use of L1 Instruction cache, Variable cache, Scalar cache.
19. What are cache Invalidation instructions?
20. [[Cache stale]]
	 - Cache staleness refers to the condition where cached data is considered outdated or expired but is still retained in the cache for potential use.
	 - caching strategies that aim to improve application performance and reliability by allowing systems to serve stale data when fresh data is unavailable.
	 - TTL(Time to Live) decides how long the content can be served fresh.

---

# Architectures

## GCN3 Architecture (Graphics Core Next) `fas:Microchip`

### Drawback

L1 and L2 caches are crossbar connected.

- Though it provides low latency and high-throughput
- the design struggles to scale with the increase in the no. of CUs.  
![[Pasted image 20240912204038.png]]

## Compute Unit

- responsible for instruction execution and data processing
- 1 Scheduler  
  can fetch and issue instructions for up to 40 wavefronts.  
  40 wavefront handling
	- decode 5 instruction  
	  and issue to 5 execution units
		- 1 branch unit
		- 1 scalar unit(executes instruction manipulating data shared by work-items in a wavefront)
		- 1 LDS(Local Data Share)
		- 1 vector memory unit
		- 4 SIMD unit
- 1 SIMD unit
	- responsible for executing vectorized floating-point instruction for 10 out 40 wavefronts
	- contains 16 single-precision ALUs
	- => 64-work-item wavefront takes 4 cycles to finish  
![[Pasted image 20240912203931.png|Architecture of a GCN3 Compute Unit]]

## Cache `fas:ShippingFast`

- two-level cache hierarchy.
- L1
	- L1 scalar cache(shared with multi CUs)  
	  used for storing constant data, such as kernel arguments and pointers.
	- L1 instruction cache(shared with multi CUs)
	- L1 vector cache  
	  write-through cache that stores most of the data  
	  Each CU has a dedicated one.
	- L1 scalar and instruction cache are shared  
	  among CUs in shader array(typically 4 CUs)
- L1 `rir:ArrowLeftS` L2(write-back caches) `rir:ArrowLeftS` DRAM (HBM or GDDR)
- Both the L2 caches and DRAM controllers are banked

## RDNA Architecture (Radeon DNA)

Designed to replace GCN for better scalability.  
![[Pasted image 20240916145837.png]]

### Major Changes

1. Reduced size of wavefront.(64`rir:ArrowRightS`32 work items)
	- It expected the CUs to better cope with higher degree of thread divergence
	- fewer memory transactions generated by 1 load/store instr.
		- The total no. of transactions generated by kernel is same.
	- potentially reducing memory acess latencies  
2. Introduction of DCUs(Dual Compute Units) replacing the GCN CUs.
	- Contains 4 schedulers.
	- 1 Scheduler dispatches instructions to 1 SIMD unit. (1 `rir:ArrowRightS` 1)
	- 1 SIMD has 32 single-precision ALUs ( 2x `fas:ExpandAlt` than CU)  
	  `fas:Equals` no. of work items  
	  => each SIMD unit can execute 1 instruction in a cycle.  
		  (compared to 4 cycles in CU)
3. 3-layer structure cache hierarchy
	- Dedicated L0I and L0S caches(read-only) for each DCU.
	- 1 DCU is connected to 2 L0V caches(write-through)  
	  => 2 SIMD and 2 Schedulers use 1 L0V.  
![[Pasted image 20240916145901.png]]  

Comparing cache latencies between the GCN and RDNA GPUs, while running the pointer chasing microbenchmark. The results suggest that, other than the publicly announced changes in the specifications (e.g., cache sizes), AMD has made many unannounced design improvements (e.g., reducing cache latencies).

![[Pasted image 20240912203601.png]]

---

# Tasks

- [ ] Go through the RDNA Whitepaper.
