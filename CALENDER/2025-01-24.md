---
Suprabhatam: false
Jogging: 
Mandir: 
Night Lab: 
Project Work:
---

# How's the ðŸŒ„ðŸŒ…ðŸŒ‡?

---

# 1% `ris:ArrowRightUp`??

---

# Tasks

---

# Reply

Sairam Sir.  
I tried installing GPGPU in the server node04. It worked without the xml. But adding the xml file results in a core dump.
- It took some time to build the environment as I was using Anaconda.
- There was some conflict between the system nvcc and the nvcc in the toolkit installed in the environment.
- Installing CUDA toolkit in my laptop didn't work as I don't have a GPU.
- So, it took some time.
- But, I think it's running completely fine for Abhinadan brother.

Next I made notes ofÂ the paper : A domain specific Supercomputer for Taining Deep Neural Networks.
- It talks about TPUv2 and v3. I had read this before, but I hadn't made any notes.
- I have attached the notes of the same.

Talking other TPU simulators in github, every other simulator just implements the basic MXU(Multiplication Unit) with MAC(multiply accumulators). Even TPTPU doen't have any method to give any input to it.

Meanwhile I have started reading the paper [Ten Lessons From Three Generations Shaped Googleâ€™s TPUv4i](https://ieeexplore.ieee.org/document/9499913)
- released in 2021 it came even before the [TPU v4: An Optically Reconfigurable Supercomputer for  Machine Learning with Hardware Support for Embeddings](https://arxiv.org/abs/2304.01433) was released. (I'm making notes of this, which is helping me in revision).
- it actually talks about the design of TPUv4 which is missing in the later.

There is some certification conflict of docker (and even for the make file) sophos, so I couldn't install the GPUTejas.

I finished reading through the **section 7.4** from Quantitative Approach which talks about Google TPUs. There wasn't much detail over there. Nonetheless it gave some basic idea about [[systolic array]] based multipliers.
- And I have started reading **Chapter 4** : Data-Level Parallelism in Vector, SIMD, and GPU Architectures.

> [!Important] Tasks Not Done
>
>```tasks
>not done
