the end of Dennard Scaling in the late 2000s as well as the end of classic Moore’s Law pace cost per transistor declines by the late 2010s.

compute capabilities have continued to improve at a rapid pace, with the baton being passed to other technologies such as [advanced packaging](https://semianalysis.com/2021/12/15/advanced-packaging-part-1-pad-limited/?access_token=eyJhbGciOiJFUzI1NiIsImtpZCI6InNlbWlhbmFseXNpcy5wYXNzcG9ydC5vbmxpbmUiLCJ0eXAiOiJKV1QifQ.eyJhdWQiOiJzZW1pYW5hbHlzaXMucGFzc3BvcnQub25saW5lIiwiYXpwIjoiS1NncVhBaGFmZmtwVjQzbmt0UU1INSIsImVudCI6eyJhdWQiOlsiNThZNVhua2U4U1ZnTkFRRm5GZUVIQiJdLCJ1cmkiOlsiaHR0cHM6Ly9zZW1pYW5hbHlzaXMuY29tLzIwMjEvMTIvMTUvYWR2YW5jZWQtcGFja2FnaW5nLXBhcnQtMS1wYWQtbGltaXRlZC8iXX0sImV4cCI6MTc1MzI1MjA1MiwiaWF0IjoxNzUwNjYwMDUyLCJpc3MiOiJodHRwczovL3NlbWlhbmFseXNpcy5wYXNzcG9ydC5vbmxpbmUvb2F1dGgiLCJzY29wZSI6ImZlZWQ6cmVhZCBhcnRpY2xlOnJlYWQgYXNzZXQ6cmVhZCBjYXRlZ29yeTpyZWFkIGVudGl0bGVtZW50cyIsInN1YiI6IjM3MzJiODk2LWIwZDUtNDZmMS04ZGQ4LWFhOWQzYTU4MTc0ZiIsInVzZSI6ImFjY2VzcyJ9.CgKx19qAZkbQiaJ_PD7oRjAhgK3jMl4gHW-cI9IOFA1m7G06qLs2KsTAAZxjvtfBPhEOyUyqY-NwfkAAzR41IQ), [3D stacking](https://semianalysis.com/2025/02/05/iedm2024/?access_token=eyJhbGciOiJFUzI1NiIsImtpZCI6InNlbWlhbmFseXNpcy5wYXNzcG9ydC5vbmxpbmUiLCJ0eXAiOiJKV1QifQ.eyJhdWQiOiJzZW1pYW5hbHlzaXMucGFzc3BvcnQub25saW5lIiwiYXpwIjoiS1NncVhBaGFmZmtwVjQzbmt0UU1INSIsImVudCI6eyJhdWQiOlsiNThZNVhua2U4U1ZnTkFRRm5GZUVIQiJdLCJ1cmkiOlsiaHR0cHM6Ly9zZW1pYW5hbHlzaXMuY29tLzIwMjUvMDIvMDUvaWVkbTIwMjQvIl19LCJleHAiOjE3NTMyNTIwNTIsImlhdCI6MTc1MDY2MDA1MiwiaXNzIjoiaHR0cHM6Ly9zZW1pYW5hbHlzaXMucGFzc3BvcnQub25saW5lL29hdXRoIiwic2NvcGUiOiJmZWVkOnJlYWQgYXJ0aWNsZTpyZWFkIGFzc2V0OnJlYWQgY2F0ZWdvcnk6cmVhZCBlbnRpdGxlbWVudHMiLCJzdWIiOiIzNzMyYjg5Ni1iMGQ1LTQ2ZjEtOGRkOC1hYTlkM2E1ODE3NGYiLCJ1c2UiOiJhY2Nlc3MifQ.zSyA6Clqwf5jE1mMIQN9RfCPKo9n3xZHiqDnSQiXJ8_Ji0D65V7uWBv0GMx3SKoG48Ozu36QfuF1O86IjTGwZg), [new transistor types](https://semianalysis.com/2023/02/21/the-future-of-the-transistor/?access_token=eyJhbGciOiJFUzI1NiIsImtpZCI6InNlbWlhbmFseXNpcy5wYXNzcG9ydC5vbmxpbmUiLCJ0eXAiOiJKV1QifQ.eyJhdWQiOiJzZW1pYW5hbHlzaXMucGFzc3BvcnQub25saW5lIiwiYXpwIjoiS1NncVhBaGFmZmtwVjQzbmt0UU1INSIsImVudCI6eyJhdWQiOlsiNThZNVhua2U4U1ZnTkFRRm5GZUVIQiJdLCJ1cmkiOlsiaHR0cHM6Ly9zZW1pYW5hbHlzaXMuY29tLzIwMjMvMDIvMjEvdGhlLWZ1dHVyZS1vZi10aGUtdHJhbnNpc3Rvci8iXX0sImV4cCI6MTc1MzI1MjA1MiwiaWF0IjoxNzUwNjYwMDUyLCJpc3MiOiJodHRwczovL3NlbWlhbmFseXNpcy5wYXNzcG9ydC5vbmxpbmUvb2F1dGgiLCJzY29wZSI6ImZlZWQ6cmVhZCBhcnRpY2xlOnJlYWQgYXNzZXQ6cmVhZCBjYXRlZ29yeTpyZWFkIGVudGl0bGVtZW50cyIsInN1YiI6IjM3MzJiODk2LWIwZDUtNDZmMS04ZGQ4LWFhOWQzYTU4MTc0ZiIsInVzZSI6ImFjY2VzcyJ9.1RaUIJnQpJgQAk3jwvJh4UfxgnryPXlfG2iZuBgc_U5egpxo_0gu9po01ABHYB5dZBAQ8FtgEcHpeYJXBse94Q) and specialized architectures such as the GPU.

![](https://ci3.googleusercontent.com/meips/ADKq_NbwfNX4lSHTMD5hyhivHTPo5Zu7Cjy_5RJQao_DwsEW7Ch5djWS2abFPDsBESc-L28f7DTl56LiFTanRLdrruN6arFTstDMymD2i8OT5aREuEiZLL0ShyaFFvVKx1GN2Yy-McYljtWYyCrjuhWxQV8zEST4V86jHg=s0-d-e1-ft#https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/06/image-151.png?resize=768%2C480&ssl=1)

Source: Nvidia

GPU compute capabilities have improved at a faster than Moore’s law pace, consistently delivering remarkable “[Huang’s Law](https://en.wikipedia.org/wiki/Huang%27s_law)” performance improvements

![](https://ci3.googleusercontent.com/meips/ADKq_NYUx9GrLFYKA0Fz6yi3su-0FgD_sAMAKVhfComqLXIK3KEbnXlS50Binn_umws87qZvjh3RCjjqswFjPIlu8KvBkKyv53B51M5Dd58MaAKCMylp53Ts1ySBqGLFk7sfcdmIEw1TT07eBFxVHbcRXle9R-2uUJZ7gaU8N2qFnRdh52-678c7Fmyf23Q=s0-d-e1-ft#https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/06/1-Nvidia-Dense-Throughput-1.png?resize=1642%2C972&ssl=1)

Source: SemiAnalysis, [HC2023-K2: Hardware for Deep Learning](https://www.youtube.com/watch?v=rsxCZAE8QNA)

## Performance First Principles

### Amdahl’s Law

For a fixed problem size, Amdahl’s Law specifies the maximum speedup you can obtain by parallelizing with more compute resources.

scaling compute resources only drives down the execution time of the parallel portion, so the performance improvement is bounded by the serial portion.

![](https://ci3.googleusercontent.com/meips/ADKq_NZKFlzxifi2pda_K0lswpgTOVrXnB41UptOpaIIzUMNk0lVM6HS3J2nO7Bve3QwZyxQt0dTi0E_l_2R23nEtOTCVss5SfqnItbJGJRtQPsZPylWJ1Fh7JeOCfd13YH5DgzhNynbCyy-FA6r61TsYcz9AnQ5DOxf-to=s0-d-e1-ft#https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/06/image-149.png?resize=1919%2C522&ssl=1)

where S is the parallel work execution time and p is the speedup of the parallelizable work.

Strong scaling refers to scaling compute resources to solve a fixed-size problem, and Amdahl’s Law quantifies the speedup of strong scaling.

weak scaling refers to scaling compute resources to solve larger problems at a constant time.

![](https://ci3.googleusercontent.com/meips/ADKq_NaD9Wuh0lQHWrunM2hf7Pn08tO1TLFGcZp1eE0ARcFWkOA8V6NXEdPOk1j9GR9Hud0-xE38-C0HpEdQotxc8gLMkglrRwQIwtp_tQBKMvh8G_ey-qjvPPmp_59uP9Q942ZHQkhyf398FfYC1vR4461C0jndLMQu6Em7odKe47RY6w=s0-d-e1-ft#https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/06/image-155-scaled.png?resize=2560%2C1807&ssl=1)

Source: SemiAnalysis, [Performance and Scalability – SCENET Summer School](https://acenet-arc.github.io/ACENET_Summer_School_General/05-performance/index.html)

Strong scaling offers speedup for all problem sizes, while weak scaling only guarantees performance improvement when we use more compute to solve a larger problem.

### Data Movement is the Cardinal Sin

Data movement is a sin because in terms of runtime and scaling, computation is cheap and data movement is expensive.

Regarding scaling, while computation speed gains have slowed since the 2000s, [memory speed has improved slower](https://semianalysis.com/2024/09/03/the-memory-wall/?access_token=eyJhbGciOiJFUzI1NiIsImtpZCI6InNlbWlhbmFseXNpcy5wYXNzcG9ydC5vbmxpbmUiLCJ0eXAiOiJKV1QifQ.eyJhdWQiOiJzZW1pYW5hbHlzaXMucGFzc3BvcnQub25saW5lIiwiYXpwIjoiS1NncVhBaGFmZmtwVjQzbmt0UU1INSIsImVudCI6eyJhdWQiOlsiNThZNVhua2U4U1ZnTkFRRm5GZUVIQiJdLCJ1cmkiOlsiaHR0cHM6Ly9zZW1pYW5hbHlzaXMuY29tLzIwMjQvMDkvMDMvdGhlLW1lbW9yeS13YWxsLyJdfSwiZXhwIjoxNzUzMjUyMDUyLCJpYXQiOjE3NTA2NjAwNTIsImlzcyI6Imh0dHBzOi8vc2VtaWFuYWx5c2lzLnBhc3Nwb3J0Lm9ubGluZS9vYXV0aCIsInNjb3BlIjoiZmVlZDpyZWFkIGFydGljbGU6cmVhZCBhc3NldDpyZWFkIGNhdGVnb3J5OnJlYWQgZW50aXRsZW1lbnRzIiwic3ViIjoiMzczMmI4OTYtYjBkNS00NmYxLThkZDgtYWE5ZDNhNTgxNzRmIiwidXNlIjoiYWNjZXNzIn0.F3dlXyWo67FelWCJblenx-JaLwZWp0yGDnmYcpdfxHMxS3HwoIzrtVl9KaLvr_DKs81Oj3BscmcXbirrfZNoRg), creating the [memory wall](https://en.wikipedia.org/wiki/Random-access_memory#Memory_wall).

## Tensor Core Architecture Evolution

### Pre-Tensor Core

#### PTX Programming Model

Parallel Thread Execution (PTX) is a virtual instruction set that abstracts over GPU generations.

PTX program describes a **kernel function** that is executed with a large number of GPU threads

**Threads** are organized as a grid, and a **grid** consists of cooperative thread arrays (**CTA**s).

threads have per-thread **registers**, threads within a CTA have **shared memory**, and all threads can access **global memory**

![](https://ci3.googleusercontent.com/meips/ADKq_Nb-x1O3uzR-KhioVj2ZCasakWi_hI30m2XqCEWnWx0Rvav0dagWb3waLGqYqm2XEw9m3pYE53bfqPACJTKu5dGJk7kHUjd44KosW0geHqzW9lj53vh8lUxVG8zHu2JgjVHGwcd83VCwEFMrJQOXrNJlf32ByMApU9gQApsrd5A6PsO9BlSBPg8=s0-d-e1-ft#https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/06/2-PTX-Programming-Model-1.png?resize=573%2C164&ssl=1)

Source: SemiAnalysis

#### PTX Machine Model

The GPU architecture is built around an array of streaming multiprocessors (**SM**s). An SM consists of scalar processing cores, a multithreaded instruction unit, and an on-chip shared memory. An SM maps each thread to a scalar processing core (also known as a CUDA core), and the multithreaded instruction unit manages threads in groups of 32 parallel threads called **warps**.

At instruction issue time, the instruction unit selects a warp and issues an instruction to the threads of the warp. This execution method is called single-instruction, multiple threads (**SIMT**).

![](https://ci3.googleusercontent.com/meips/ADKq_NaFNY9UTeXw3E70Hp_5ico1_KeDv-a3u2T30OxSyvv3QeXXOIwO9nK5M4wRCl1KHzaEkYMWyN4RY8ajwa7PI1sW9AeeEhp7MLPOP14dzjLo7Z-3Y781i29lPQGCX8FxVKR15xkQcP_YoFZekWrCChTvv6DrllYGxui8PelbwA=s0-d-e1-ft#https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/06/machine_model.png?resize=1606%2C1630&ssl=1)

PTX Machine model. Source: SemiAnalysis, [PTX ISA Documentation – Figure 4](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html#set-of-simt-multiprocessors-hardware-model)

#### Streaming Assembler

Streaming Assembler (SASS) is the architecture-specific instruction set that PTX virtualizes over.

### Volta

#### Why NVIDIA Added Tensor Cores

Early in 2015, Google designed the TPUv1 for accelerating matrix multiplication, and around the same timeframe, Nvidia also started developing dedicated hardware for matrix math.

GPUs consume a small amount of energy when issuing instructions (~30pJ) because of their simple hardware pipeline, simple floating point operations like `==HFMA== `consume even less energy at only 1.5pJ.

This creates a 20x overhead of power needed for instructions vs for the floating point operation itself.

To amortize the instruction overhead, we need to use complex instructions that can perform more computation per instruction.

half-precision matrix multiply and accumulate (`==HMMA==`) instruction

The corresponding dedicated hardware to execute this instruction is the Tensor Core, introduced in the Tesla V100 GPU of Volta architecture in 2017.

![](https://ci3.googleusercontent.com/meips/ADKq_NatfuIzrmXtSCRw3wryxc0olcg6yTl9mE2B2hzt5vxk73NgHDmr8daAfOkrsfaaH5uFN1LYvTVzq-VgiUZnkA5R20pLLKHmUeLOISbqulK6epek8MQevjwtdyMUvEWD2g8H1p3Eq2ZT0lIlfhtFambcfgvU6hBK91FSOSWa0Aya=s0-d-e1-ft#https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/06/3-SASS-Operations.png?resize=797%2C226&ssl=1)

Source: [Trends in Deep Learning Hardware: Specialized Instructions Amortize Overhead](https://www.youtube.com/watch?v=kLiwvnr4L80&t=869)

#### MMA Instruction Overview

Given a matrix, the multiply and accumulate (MMA) instruction computes D = A \* B + C

- A is an M by K matrix
- B is a K by N matrix
- C and D are M by N matrices

We denote the matrix shapes as `==mMnNkK==` or MxNxK.

The full computation is collectively performed by multiple threads, meaning that every step requires a synchronization between the collaborating threads.

#### 1st Generation Tensor Core – Warp-scoped MMA

An SM of a Tesla V100 GPU contains 8 Tensor Cores, grouped in partitions of two.

Each Tensor Core is capable of computing an equivalent of 4x4x4 matrix multiplication per cycle, which amounts to 1024 FLOPs per cycle per SM

![](https://ci3.googleusercontent.com/meips/ADKq_NYhDdeiKuvm-zshew487p2UruvwMGTc4YV5agf9KKAZDr_s7B2Yr_DjMj57ghhpobSvXO5AxraFGobkqBCFa-2m3vwbfSB_6g0bjdxlvJ7vy8DhhrFr7JNcVhpWuWyc_IC2t8r2G38WAFv_orjWpRrF4xbAnQoi_BT2qRvggFtkqg=s0-d-e1-ft#https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/06/3c-Volta-MMA-SYNC.png?resize=1398%2C784&ssl=1)

[Source: Volta Tensor Core Training](https://www.olcf.ornl.gov/wp-content/uploads/2019/11/ORNL_Tensor_Core_Training_Aug2019.pdf)

NVIDIA designed PTX instruction mma to target the lower level `==HMMA==` instructions.

On Volta architecture, an MMA instruction performs an 8x8x4 matrix multiplication, and a quadpair of 8 threads participate in the operation by collectively holding the input and output matrices.

![](https://ci3.googleusercontent.com/meips/ADKq_NZDKKnq-wRYBb4Gmdn_OwontmLL37hVlKnfFvHuvHtDibR_p50Ux7uNDVaz4ZR0FglI3TOAr7zttARYswug5NaN5S7K4CPhY0-w-1BHxX3ug740RYWZw_hzn38MLH8xWOlnfM2nfWv97mwD4S56c5VcCixyb9TuhLbsJTji_02_9lewEeEL=s0-d-e1-ft#https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/06/3d-CUTLASS-visualizer-1.png?resize=983%2C975&ssl=1)

Source: SemiAnalysis. Generated with CUTLASS visualizer

Volta Tensor Cores support FP16 inputs with FP32 accumulation in correspondence with NVIDIA’s [mixed-precision training](https://arxiv.org/abs/1710.03740) technique.

To fully understand the MMA layout, please refer to Citadel’s microbenchmarking paper, [Dissecting the NVIDIA Volta GPU Architecture via Microbenchmarking](https://arxiv.org/abs/1804.06826). To see the interleaved layout pattern for Volta Tensor Core MMAs, please read the slides [Programming Tensor Cores: Native Tensor Cores with CUTLASS](https://developer.download.nvidia.com/video/gputechconf/gtc/2019/presentation/s9593-cutensor-high-performance-tensor-operations-in-cuda-v2.pdf). Finally, for other information of the Volta architecture, please refer to the whitepaper [NVIDIA Tesla V100 GPU Architecture](https://images.nvidia.com/content/volta-architecture/pdf/volta-architecture-whitepaper.pdf).

### Turing

Turing architecture includes the **2nd generation Tensor Cores**, an enhanced version of Volta Tensor Cores, adding INT8 and INT4 precision support.

Turing Tensor Cores support a new warp-level synchronous MMA

Turing Tensor Cores also enabled Deep Learning Super Sampling (DLSS), marking the start of NVIDIA applying deep learning to gaming graphics.

Interested readers can refer to NVIDIA’s blog post [NVIDIA Turing Architecture In-Depth](https://developer.nvidia.com/blog/nvidia-turing-architecture-in-depth/) and the [Turing architecture whitepaper](https://images.nvidia.com/aem-dam/en-zz/Solutions/design-visualization/technologies/turing-architecture/NVIDIA-Turing-Architecture-Whitepaper.pdf).

### Ampere

#### Asynchronous Data Copy

With Ampere, NVIDIA introduced asynchronous data copy, a way of copying data directly from global memory to shared memory in an asynchronous fashion.

To load data from global memory to shared memory on Volta, threads must first load data from global memory to registers, and then store it to shared memory.

by fetching data from global memory (DRAM) and directly storing it into shared memory (with optional L1 access), freeing up more registers for MMA instructions.

Data loading and compute can happen asynchronously which is more difficult from a programming model perspective but unlocks higher performance.

This feature is implemented as PTX instruction thread-level async copy cp.async

The corresponding SASS is LDGSTS, asynchronous global to shared memory copy

exact synchronization methods are async-group and mbarrier-based completion mechanisms, detailed [here](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html?highlight=tcgen05%2520cp#data-movement-and-conversion-instructions-asynchronous-copy-completion-mechanisms).

![](https://ci3.googleusercontent.com/meips/ADKq_NapPZiBMaD683mMEM6Oq2l0QmfFRp6IlfYqXX5hNIO9HDYwQyj4ExoUTM8LvvULcLpKI5xOJvu4KeTx_Iovv03WAqrms4J8V49cyI7Vk0vWtV44E1Ej_KHwI-fwOTnhDSOYlu13jqeuU6vw8M7A_EL-sk8e8vBkry6L_yNWAUTwuEF-rJwmZ4JJXw=s0-d-e1-ft#https://i0.wp.com/semianalysis.com/wp-content/uploads/2025/06/3e-Without-Async-Copy-pg62.png?resize=1603%2C339&ssl=1)

Source: [NVIDIA A100 Tensor Core GPU Architecture Whitepaper](https://images.nvidia.com/aem-dam/en-zz/Solutions/data-center/nvidia-ampere-architecture-whitepaper.pdf)

#### 3rd Generation Tensor Core – Warp-level Synchronous MMA

Ampere has 4 Tensor Cores per SM, and each Tensor Core is capable of performing 512 FLOPs per cycle

doubling the performance of Volta.

Volta requires a quadpair of 8 threads to participate in an MMA operation

Having MMA instructions warp-wide simplifies the thread layout & reducing RF pressure for Ampere.